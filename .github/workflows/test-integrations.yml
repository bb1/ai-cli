name: Test Integrations

on:
  push:
    branches:
      - main
    paths:
      - 'src/providers/**'
      - 'scripts/test-integrations.ts'
      - '.github/workflows/test-integrations.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'src/providers/**'
      - 'scripts/test-integrations.ts'
      - '.github/workflows/test-integrations.yml'

jobs:
  test-ollama-lmstudio:
    name: Test Ollama and LM Studio Integrations
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        env:
          OLLAMA_HOST: 0.0.0.0:11434
        options: >-
          --health-cmd "curl -f http://localhost:11434/api/tags || exit 1"
          --health-interval 30s
          --health-timeout 10s
          --health-retries 10
          --health-start-period 60s

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2

      - name: Install dependencies
        run: bun install

      - name: Pull Ollama model
        run: |
          # Wait for Ollama service to be ready with extended timeout
          for i in {1..120}; do
            if curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "Ollama is ready!"
              break
            fi
            echo "Waiting for Ollama... (attempt $i)"
            sleep 2
          done
          
          # Pull a very small model (tinyllama is ~637MB, designed for CPU inference)
          echo "Pulling TinyLlama model for Ollama..."
          curl -X POST http://localhost:11434/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "tinyllama"}' \
            --max-time 600 \
            --connect-timeout 30

      - name: Start LM Studio
        run: |
          # LM Studio doesn't have an official Docker image, so we'll use a compatible OpenAI-compatible server
          # We'll use ollama with a different port for LM Studio simulation
          docker run -d \
            --name lmstudio-compat \
            -p 1234:11434 \
            ollama/ollama:latest

          # Wait for the service to be ready with extended timeout
          for i in {1..120}; do
            if curl -sf http://localhost:1234/api/tags > /dev/null 2>&1; then
              echo "LM Studio compatible server is ready!"
              break
            fi
            echo "Waiting for LM Studio... (attempt $i)"
            sleep 2
          done
          
          # Pull the same model for LM Studio
          echo "Pulling model for LM Studio..."
          curl -X POST http://localhost:1234/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "tinyllama"}' \
            --max-time 600 \
            --connect-timeout 30

      - name: Verify services are ready
        run: |
          echo "Verifying all services are healthy..."
          
          # Check Ollama
          for i in {1..10}; do
            if curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "✓ Ollama is ready"
              break
            fi
            if [ $i -eq 10 ]; then
              echo "✗ Ollama failed to become ready"
              exit 1
            fi
            sleep 2
          done
          
          # Check LM Studio
          for i in {1..10}; do
            if curl -sf http://localhost:1234/api/tags > /dev/null 2>&1; then
              echo "✓ LM Studio is ready"
              break
            fi
            if [ $i -eq 10 ]; then
              echo "✗ LM Studio failed to become ready"
              exit 1
            fi
            sleep 2
          done
          
          echo "All services verified!"

      - name: Run integration tests
        env:
          OLLAMA_URL: http://localhost:11434
          OLLAMA_MODEL: tinyllama
          LMSTUDIO_URL: http://localhost:1234
          LMSTUDIO_MODEL: tinyllama
        run: bun run scripts/test-integrations.ts

      - name: Cleanup
        if: always()
        run: |
          docker stop lmstudio-compat || true
          docker rm lmstudio-compat || true
